import sys, os
sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定
import numpy as np
from dataset.mnist import load_mnist
from common.util import *

# 超参数的验证
# 那些数据是超参数，如，各层的神经元数量、batch大小、参数更新时的 学习率 或 权值衰减系数 等
# 如果超参数的值没有设置成合适的值，模型的性能就会很差。

#--------------------------------------------------------

# 验证数据
# 这里为了后面测试 超参数 性能好坏，需要在 训练数据、测试数据 之外额外再准备一个 验证数据，这个 验证数据 是专门用来验证测试数据好坏的
# 训练数据，用于训练模型，用于模型的学习
# 测试数据，用于评估模型的泛化能力，检查模型的学习结果，是否过度拟合了 训练数据
# 验证数据，用于调整 超参数 的数据，验证 超参数 性能的数据
# 为啥要专门准备一个 验证数据 来检查超参数性能，而不能用 测试数据 检查超参数的性能？
# 答：如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。

# 因为 MNIST数据集 没有验证数据，所以需要从 训练数据中抽取20%，作为验证数据
(x_train, t_train), (x_test, t_test) = load_mnist()
# 打乱训练数据
# 这里打乱数据，是因为，数据集的数据可能存在偏向（比如，数据从“0”到“10”按顺序排列等）
x_train, t_train = shuffle_dataset(x_train, t_train)
# 分割验证数据
validation_rate = 0.20
validation_num = int(x_train.shape[0] * validation_rate)
print(validation_num)
x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]

#--------------------------------------------------------

#超参数最优化的步骤：

#步骤0
#设定超参数的范围。

#步骤1
#从设定的超参数范围中随机采样。

#步骤2
#使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）。

#步骤3
#重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。

#反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。

#这里介绍的超参数的最优化方法是实践性的方法。不过，这个方法与其说是科学方法，倒不如说有些实践者的经验的感觉。在超参数的最优化中，如果需要更精炼的方法，可以使用 贝叶斯最优化。

#--------------------------------------------------------

#超参数最优化的实现
# 现在，我们使用MNIST数据集进行超参数的最优化。
# 这里我们来寻找 学习率 和 权值衰减系数 它们俩的最优参数。
# 详细请看，/demo6/hyperparameter_optimization.py
# 从上面的测试中，我们得到了，如下数据（按识别精度从高到低的顺序排列了验证数据的学习的变化）
'''
Best-1(val acc:0.83) | lr:0.007834694339073335, weight decay:3.006917362802703e-07
Best-2(val acc:0.78) | lr:0.009809466235968813, weight decay:2.0859987864466797e-07
Best-3(val acc:0.76) | lr:0.006008412084388165, weight decay:2.879110624370979e-08
Best-4(val acc:0.72) | lr:0.007836210687057449, weight decay:9.74941566521056e-07
Best-5(val acc:0.66) | lr:0.005704351360916943, weight decay:2.0274792090324133e-06
...
'''
# 从上面的数据可知，直到“Best-5”左右，学习进行得都很顺利。因此，我们来观察一下“Best-5”之前的超参数的值（学习率和权值衰减系数），总结如下：
# 从这个结果可以看出，学习率在0.001到0.01、权值衰减系数在 10的−8次幂 到 10的−6次幂 之间时，学习可以顺利进行。
# 然后，咱们按照，上面得出来的结果，重新设置 学习率 和 权值衰减系数 的范围，然后重复上面的操作，再次得新结果，再次缩小范围，然后在某个阶段，选择一个最终的超参数的值。这样我们就找到了最优的超参数





