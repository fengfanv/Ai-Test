# Batch Norm
# 在上一章，我们学习到，如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。
# 那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上，Batch Normalization方法就是强制性调整各层激活值分布的方法。
# Batch Norm的优点，
# 1、可以使学习快速进行（可以增大学习率）
# 2、不那么依赖初始值（对于初始值不用那么神经质）
# 3、抑制过拟合（降低Dropout等的必要性）
# 深度学习的学习过程，要花费很多时间，第一个优点令人非常开心。另外，后两点也可以帮我们消除深度学习的学习中的很多烦恼。
# Batch Norm的思路是调整各层的激活值分布使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行正规化的层，即Batch Norm层
# 如下是神经网络：
# Affine ReLU (BatchNorm) 神经网络第一层
# Affine ReLU (BatchNorm) 神经网络第二层
# Affine Softmax 神经网络输出层

# 观察使用Batch Norm层和不使用Batch Norm层时学习的过程会如何变化，请看如下文件，/demo6/batch_norm_test.py
